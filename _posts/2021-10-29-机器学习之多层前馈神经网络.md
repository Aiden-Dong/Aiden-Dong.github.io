---
layout:     post
title:      机器学习篇 | BP 神经网络引入
subtitle:   
date:       2021-10-29
author:     Aiden
header-img: img/post-bg-coffee.jpeg
catalog: true 
tags:
    - 机器学习
---


**BP 神经网络**是指使用**BP算法**训练的**前馈神经网络**, 神经网络模型形如:

![image.png]({{ site.url }}/assets/ml_5_1.jpg)

神经网络基本分位三部分 : 

- **输入层**对接样本的特征向量
- 中间包含0到多个**隐含层**
- 输出层对应预测结果

神经网络中的每一个神经节点都是一个神经元

![image.png]({{ site.url }}/assets/ml_5_2.jpg)

常见的激活函数有`sigmod`, `ReLU`, `tanh`

单层神经网络主要用来解决线性可分的问题， 对于不可分的问题， 采用多层神经网络

#### BP 神经网络训练与预测

![image.png]({{ site.url }}/assets/ml_5_3.jpg)

给定训练集 $D= \lbrace  (x_1, y_1),(x_2, y_2), ..., (x_m, y_m) \rbrace , x_i \in \mathbb{R}^{d}, y_i \in \mathbb{R}^{l}$ ,
即输入的样本有 $d$ 个特征值, 输出 $l$ 为实值向量。 在上面模型中, 我们定义了具有 $q$ 个神经元的单隐层, 在每层添加一个值为 $1$ 的常量来起到阈值的作用，这样就可以将阈值放进权重矩阵内部。

所以就有隐层第 $h$ 个神经元`接受到的输入`为 $\alpha_h = \sum_{i=1}^{d+1}v_{ih}x_{i}, x_{d+1}=1$, 输出层第 $j$ 个神经元`接受到的输入`为 $\beta_j = \sum_{h=1}^{q+1} w_{hj}b_h, b_{q+1}=1$

对于**激活函数我们采用 sigmod**, **损失函数采用均方差**。

对于样本 $(x_k, y_k)$ , 均方差为 : 

$$E^{k}_{j} = \frac{1}{2} \sum_{j=1}{l} (\hat{y}^{k}_{j} - y^{k}_{j})^2$$

##### 链式求导法则 : 


\frac{\partial E^{k}_{j}}{\partial w_{hj}} = \frac{\partial E^{k}_{j}}{\partial \hat{y}^{k}_{j}} \cdot \frac{\partial \hat{y}^{k}_{j}}{\partial \beta_{j}} \cdot \frac{\partial \beta_{j}}{\partial w_{hj}}











---


> 说明

- 代码采用特征为连续属性莺尾花样本集
- 参考 [周志华-机器学习]