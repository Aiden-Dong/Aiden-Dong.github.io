---
layout:     post
title:      机器学习篇 | 分类模型-逻辑回归 (Logistic Regression)
subtitle:   
date:       2021-10-18
author:     Aiden
header-img: img/post-bg-coffee.jpeg
catalog: true 
tags:
    - 机器学习
---

简单来说， 逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。
比如某用户购买某商品的可能性，某病人患有某种疾病的可能性，以及某广告被用户点击的可能性等。

逻辑回归虽然称为回归，实则是一个二分类模型。 

### sigmod 函数 

逻辑回归的核心为**sigmod** 函数 :

$$h_{\theta}(x) = \frac{1}{1+e^{-z}}$$

![image.png]({{ site.url }}/assets/ml_2_2.jpg)

对于逻辑回归任务，样本集可以分类正负两种样本类型，例如对于识别猫类型的图片样本， 对于是猫图片可以定义为正向样本(值为1)， 对于非猫照片可以定义为负向样本(值为0).

在理想情况下，当模型应用到样本里面时 : 

- 对于正向样本 : $h_{\theta}(x) \to 1 (z \to -\infty)$
- 对于负向样本 : $h_{\theta}(x) \to 0 (z \to +\infty)$

对于参数 $z$ : 

$$z=\theta^{T}x$$

- $\theta$ 为模型参数
- $x$ 为样本的特征向量


### 模型

对于数据样本， 我们取 $y$ 标识样本实际的类型:

- 对于正向样本 $y = 1$
- 对于负向样本 $y = 0$

在理想情况下， 我们期望训练得到一个好的模型，对于任意的$x$, 都有$[y-h_{\theta}(x)]\to 0$

#### 建立模型

我们可以用概率角度来思考这个问题， 上述问题可以有 :

$$\begin{cases}
p(y=1|x;\theta) = h_{\theta}(x)\\
p(y=0|x;\theta) = 1- h_{\theta}(x)\\
\end{cases}
$$

统一一下， 利用0-1分布的样式表示: 

$$p(y|x;\theta) = h_{\theta}(x)^y[1-h_{\theta}(x)]^{(1-y)}$$

我们只需要寻找到合适的$\theta$, 最大化这个概率模型。通过这个思路来寻找一个合适的 $h_{\theta}(x)$，
这便是一个极大似然问题，引入似然函数: 

$$L(\theta) = \prod_{i=1}^{m}h_{\theta}(x_i)^{y_{i}}[1-h_{\theta}(x_i)]^{(1-y_i)}$$

转对数似然 : 

$l(\theta) = \ln L(\theta) = \sum_{i=1}^{m} \lbrace y_{i}\ln h_{\theta}(x_i) + (1-y_{i})\ln [1-h_{\theta}(x_i)] \rbrace$

所以演变为一个优化问题, 只需要 $\max{l(\theta)}$

#### 定义损失函数

我们使用梯度下降法来解决这个问题 : 

定义损失函数  : $J(\theta) = -\frac{1}{m}l(\theta)$

#### 梯度下降法

关于梯度下降的公式推导 : 

$$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m} \lbrace y_{i}\ln h_{\theta}(x_i) + (1-y_{i})\ln[1-h_{\theta}(x_i)] \rbrace$$


$$\frace{\partial J(\theta)}{\partial \theta} = - \frace{1}{m} \sum{1}{m} \lbrace y_{i} \frace{1}{h_{\theta}(x_{i})} \frace{\partial h_{\theta}(x_{i})}{\partial \theta} - (1-y_{i})\frace{1}{1-h_{\theta}(x_{i})} \frace{\partial h_{\theta}(x_{i})}{\partial \theta} \rbrace$$






