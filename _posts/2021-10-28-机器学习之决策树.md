---
layout:     post
title:      机器学习篇 | 决策树介绍
subtitle:   
date:       2021-10-28
author:     Aiden
header-img: img/post-bg-coffee.jpeg
catalog: true 
tags:
    - 机器学习
---


决策树（Decision Tree）是一种简单但是广泛使用的分类器。通过训练数据构建决策树，可以高效的对未知的数据进行分类。

决策数有两大优点:

- 决策树模型可以读性好，具有描述性，有助于人工分析；
- 效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。


### 决策树的基本流程

![image.png]({{ site.url }}/assets/ml_4_1.jpg)

如上所示， 在基于递归模式的划分属性过程中， 在遇到以下三种情况会阻止递归的向下传递: 

1. 当前样本同属于一个类别，此时无需继续划分
2. 当前属性集为空，无法继续划分属性。（此时用含有样本数最多的一个类别作为该划分的最终分类类型）
3. 当前属性集为空，则无需划分

### 划分属性算法

属性划分算法帮助我们基于训练样本集选择一个基于样本特征最优的属性分类方案，
帮助我们基于样本特征简单的将样本分类。
常见的划分方式有基于信息熵的`ID3`与`C4.5`算法，基于基尼指数的`CART`算法。


##### 信息熵

信息熵是度量样本集合纯度最常用的一种指标, 假定当前样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k(k=1,2,...,y)$ ,则 $D$ 的信息熵定义为 : 

$$Ent(D) = - \sum_{k=1}^{y}p_k \log_2{p_k}$$

$Ent(D)$ 的值越小，则 $D$ 的**纯度**越高

##### 基尼指数

基尼指数使用基尼值来度量数据集 $D$ 的纯度 :

$$Gini(D) = \sum_{k=1}^{y}\sum_{j \neq k}p_{k}p_{j} = 1 - \sum_{k=1}^{y}p_{k}^{2}$$

直观来说，$Gini(D)$ 反映了从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率。
因此, $Gini(D)$ 越小， 则数据集 $D$ 的纯度越高。

#### 基于信息增益的 ID3 算法

ID3 使用信息增益来衡量计算



#### 基于信息增益率的 C4.5 算法

#### 基于基尼指数的 CART 算法

### 离散特征与连续特征的处理方式

### 剪枝技术防止过拟合

