---
layout:     post
title:      机器学习篇 | 决策树介绍
subtitle:   
date:       2021-10-28
author:     Aiden
header-img: img/post-bg-coffee.jpeg
catalog: true 
tags:
    - 机器学习
---


决策树（Decision Tree）是一种简单但是广泛使用的分类器。通过训练数据构建决策树，可以高效的对未知的数据进行分类。

决策数有两大优点:

- 决策树模型可以读性好，具有描述性，有助于人工分析；
- 效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。


### 决策树的基本流程

![image.png]({{ site.url }}/assets/ml_4_1.jpg)

如上所示， 在基于递归模式的划分属性过程中， 在遇到以下三种情况会阻止递归的向下传递: 

1. 当前样本同属于一个类别，此时无需继续划分
2. 当前属性集为空，无法继续划分属性。（此时用含有样本数最多的一个类别作为该划分的最终分类类型）
3. 当前属性集为空，则无需划分

### 划分属性算法

属性划分算法帮助我们基于训练样本集选择一个基于样本特征最优的属性分类方案，
帮助我们基于样本特征简单的将样本分类。
常见的划分方式有基于信息熵的`ID3`与`C4.5`算法，基于基尼指数的`CART`算法。


##### 信息熵

信息熵是度量样本集合纯度最常用的一种指标, 假定当前样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k(k=1,2,...,y)$ ,则 $D$ 的信息熵定义为 : 

$$Ent(D) = - \sum_{k=1}^{y}p_k \log_2{p_k}$$

$Ent(D)$ 的值越小，则 $D$ 的**纯度**越高


```
def information_entropy(X:np, Y:np) -> float :
    """
    信息熵
    :param X:
    :param Y:
    :return:
    """

    sample_count = len(Y)

    ent = 0.0

    for target in np.unique(Y):
        target_sample_count = len(Y[Y == target])
        target_probabili = target_sample_count / sample_count
        ent += ((target_probabili) *  -math.log(target_probabili, 2))

    return ent
```

##### 基尼值

基尼值来度量数据集 $D$ 的纯度的算法表示:

$$Gini(D) = \sum_{k=1}^{y}\sum_{j \neq k}p_{k}p_{j} = 1 - \sum_{k=1}^{y}p_{k}^{2}$$

直观来说，$Gini(D)$ 反映了从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率。
因此, $Gini(D)$ 越小， 则数据集 $D$ 的纯度越高。

```
def gini_index(X:np, Y:np) -> float :
    """
    基尼基数
    :param X:
    :param Y:
    :return:
    """
    sample_count = len(Y)

    gini = 1

    for target in np.unique(Y) :

        sample_target_count = len(Y[Y == target])

        sample_target_ratio = sample_target_count / sample_count

        gini = gini - sample_target_ratio ** 2

    return gini
```

#### 基于信息增益的 ID3 算法

ID3 使用信息增益为准则来选择划分属性。

对于样本集 $D$ , 如果基于属性 $a$ 来划分，将样本集 $D$ 划分为 $V$ 类，每个子样本集为 $D^{v}, (v=1,2,...V)$.

则样本集 $D$ 对于属性 $a$ 划分 的信息增益为 : 

$$Gain(D,a) = Ent(D) - \sum_{v=1}^{V} \frac{D^v}{D} Ent(D^v)$$


一般而言，信息增益越大，则意味着使用属性 $a$ 来进行划分所获得的**纯度提升**越大。

因此对于信息增益的选择方式，我们选择**信息增益最大**属性来划分 $a_* = arg \max Gain(D,a)$


```
def ID3(X:np, Y:np) -> tuple :
    """
    ID3 算法， 使用信息增益
    :param X:
    :param Y:
    :return:
    """

    feature_num = X.shape[1]

    # sample_info_ent = information_entropy(X, Y) # 计算信息熵

    min_info_ent = math.inf
    min_item = 0
    min_feature_cut = 0


    for feature_item in range(feature_num) :
        feature_sample_unique = np.sort(np.unique(X[:, feature_item]))

        for feature_item_index in range(len(feature_sample_unique) -1):

            feature_item_cut = (feature_sample_unique[feature_item_index] + feature_sample_unique[feature_item_index + 1])/2

            x1 = X[X[:, feature_item] < feature_item_cut]
            y1 = Y[X[:, feature_item] < feature_item_cut]

            pa_sample_1 = len(y1) / len(Y)

            ent1 = information_entropy(x1, y1)

            x2 = X[X[:, feature_item] > feature_item_cut]
            y2 = Y[X[:, feature_item] > feature_item_cut]
            pa_sample_2 = len(y2) / len(Y)

            ent2 = information_entropy(x2, y2)

            gain = pa_sample_1 * ent1 + pa_sample_2 * ent2

            if gain < min_info_ent :
                min_info_ent = gain
                min_item = feature_item
                min_feature_cut = feature_item_cut

    return min_item, min_feature_cut
```

#### 基于信息增益率的 C4.5 算法

信息增益准则对取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响， `C4.5` 算法不直接使用信息增益， 而是使用**信息增益率**来选择最优划分属性。

信息增益率的公式为 : 

$$Gaom_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$

其中

$$IV(a) = - \sum_{v=1}^{V} \frac{D^v}{D}\log_{2}\frac{D^v}{D}$$

增益率对取值数目较少的属性会有所偏好，因此， `C4.5`算法并不是直接选择增益率最大的候选划分属性，而是是用了一个启发式: **先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的**。

```
def C4_5(X:np, Y:np) -> tuple :
    """
    C4.5 信息增益率算法
    为了逻辑简单直接选择增益率最高的
    :param X:
    :param Y:
    :return:
    """

    feature_num = X.shape[1]

    sample_info_ent = information_entropy(X, Y) # 计算信息熵

    max_gain_ratio = 0
    max_item = 0
    max_feature_cut = 0

    for feature_item in range(feature_num):
        feature_sample_unique = np.sort(np.unique(X[:, feature_item]))

        for feature_item_index in range(len(feature_sample_unique) - 1):

            feature_item_cut = (feature_sample_unique[feature_item_index] + feature_sample_unique[
                feature_item_index + 1]) / 2

            x1 = X[X[:, feature_item] < feature_item_cut]
            y1 = Y[X[:, feature_item] < feature_item_cut]

            pa_sample_1 = len(y1) / len(Y)
            info_ent1 = information_entropy(x1, y1)

            x2 = X[X[:, feature_item] > feature_item_cut]
            y2 = Y[X[:, feature_item] > feature_item_cut]

            pa_sample_2 = len(y2) / len(Y)
            info_ent2 = information_entropy(x2, y2)


            gain = sample_info_ent - (pa_sample_1 * info_ent1 + pa_sample_2 * info_ent2)

            iv = - math.log(pa_sample_1, 2) - math.log(pa_sample_2, 2)

            gain_ratio = gain / iv

            if gain_ratio > max_gain_ratio:
                max_gain_ratio = gain_ratio
                max_item = feature_item
                max_feature_cut = feature_item_cut

    return max_item, max_feature_cut
```

#### 基于基尼指数的 CART 算法

`CART`决策树使用基尼指数来选择划分属性, 对应属性 $a$ 的基尼指数定义为  :

$$Gini_index(D,a) = \sum_{v=1}^{V} Gini(D^v)$$

于是， 我们在候选属性 $a$ 中，选择那个使得划分后基尼指数最小的属性最为最优划分属性， 即 $a_* = arg \max Gini_index(D,a)$

### 离散特征与连续特征的处理方式

### 剪枝技术防止过拟合

