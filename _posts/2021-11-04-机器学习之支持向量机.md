---
layout:     post
title:      机器学习篇 | 支持向量机svm
subtitle:   
date:       2021-11-04
author:     Aiden
header-img: img/post-bg-coffee.jpeg
catalog: true 
tags:
    - 机器学习
---

### 引入 


![image.png]({{ site.url }}/assets/ml_6_1.jpg)


svm 解决二分类问题, 对于样本的向量空间分布集, svm 旨在要寻找一个分割面，将样本集按照分类标签正确的分割开来。我们称这个分割平面为**分离超平面**。

假设空间样本集是可分割的， 那么总存在无数个超平面可以将样本集分割， 如何才能找到一个最优的超平面？ 

**svm 的目标是找一个最优超平面，使得距离超平面最近的点的间隔距离最大化。 这个距离超平面最近的点就是支持向量。**


首先定义特征空间的训练样本集 : $T=\lbrace (x_1, y_1), (x_2, y_2), ..., (x_N, y_N)  \rbrace$

其中， $x_i \in \mathbb{R}^{n}, y_{i} \in \lbrace -1, 1 \rbrace, i = 1, 2, ..., N$ , $x_i$ 为第 $i$ 个特征向量， $y_i$ 为类标记，当它等于 $+1$ 时为正例；为 $-1$ 为负例。再假设训练数据集是线性可分的。


假设存在一个超平面 $w^{*T}x+b^{*} = 0$ , 设其距离支持向量 $(x_i, y_i)$ 的距离为 $\gamma$ .

则有 $\frac{ y_i ( w^{*T}x_i + b^{*} ) }{ \begin{Vmatrix} w^* \end{Vmatrix}}  = \gamma$ ,  两边同除 $\gamma$ , ( $y_i$ 保证正号 )
则有 $\frac{ y_i ( w^{*T}x_i + b^{*} ) }{ \gamma \begin{Vmatrix} w^* \end{Vmatrix} }  = 1$

引入 $w = \frac{ w^* }{ \gamma \begin{Vmatrix} w^* \end{Vmatrix} }$ , $b = \frac{ b^* }{ \gamma \begin{Vmatrix} w^* \end{Vmatrix} }$

则有 $y_i ( w^Tx_i + b ) = 1$ ,此时距离支持向量的距离为 $y_i ( \frac{ w^Tx_i + b }{ \begin{Vmatrix} w \end{Vmatrix} } ) = \frac { 1 }{ \begin{Vmatrix} w \end{Vmatrix} }$
 
我们想要到支持向量的距离最大化， 而这时候对于空间中任意样本 $y ( w^T x + b ) \geq 1$ ,  

引出目标函数 : 

$$\min \frac{1}{2}  \begin{Vmatrix} w \end{Vmatrix} ^2 $$

限制条件 :
 
$$1-y_i(w^T x_i +b) \leq 0 ,  i = 1,2,...,N$$

### 软间隔与正则化

### 对偶

### SMO 

---


> 说明

- 代码采用特征为连续属性莺尾花样本集
- 参考 [周志华-机器学习]